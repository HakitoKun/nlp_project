{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d82ba35-1b51-476f-afea-0ed05b4686d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa84658a-376c-43a5-b1b5-703452c427da",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = tfds.builder(\"scientific_papers\", data_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "899c4665-0c9c-49ac-8423-a473b55421e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e92a6c17-aeb2-4046-8123-530fe06da95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = builder.as_dataset(split=\"train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020ca166-ca56-4e86-b9f2-128925518d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {abstract: (), article: (), section_names: ()}, types: {abstract: tf.string, article: tf.string, section_names: tf.string}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f76854a-7cb5-40b2-abd5-1d77a0e298fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c7ae5c-bdda-4316-b036-c2b08ad0793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: {abstract: (), article: (), section_names: ()}, types: {abstract: tf.string, article: tf.string, section_names: tf.string}>\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "num_elements = tf.data.experimental.cardinality(ds).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad793cc6-d735-470f-8515-dc8156253a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elements = tf.data.experimental.cardinality(ds).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659157c7-b749-4546-a46a-4f5450ee478f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203037"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_elements # 203037 elements au total dans le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3939c9f-6876-45c0-8473-d09be1c9683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in ds:\n",
    "    num_elements += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536edfec-4275-471a-ae0a-2e107fa5b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b' efficient techniques to navigate networks with local information are fundamental to sample large - scale online social systems and to retrieve resources in peer - to - peer systems . \\n biased random walks , i.e. walks whose motion is biased on properties of neighbouring nodes , have been largely exploited to design smart local strategies to explore a network , for instance by constructing maximally mixing trajectories or by allowing an almost uniform sampling of the nodes . here \\n we introduce and study biased random walks on multiplex networks , graphs where the nodes are related through different types of links organised in distinct and interacting layers , and we provide analytical solutions for their long - time properties , including the stationary occupation probability distribution and the entropy rate . \\n we focus on degree - biased random walks and distinguish between two classes of walks , namely those whose transition probability depends on a number of parameters which is extensive in the number of layers , and those whose motion depends on intrinsically multiplex properties of the neighbouring nodes . \\n we analyse the effect of the structure of the multiplex network on the steady - state behaviour of the walkers , and we find that heterogeneous degree distributions as well as the presence of inter - layer degree correlations and edge overlap determine the extent to which a multiplex can be efficiently explored by a biased walk . finally we show that , in real - world multiplex transportation networks , the trade - off between efficient navigation and resilience to link failure has resulted into systems whose diffusion properties are qualitatively different from those of appropriately randomised multiplex graphs . \\n this fact suggests that multiplexity is an important ingredient to include in the modelling of real - world systems .    \\n the network paradigm has proven to be a successful framework to study the intricate patterns of relations among the constituents of real - world complex systems , from the internet to the human brain  @xcite , and has revealed that the dynamical behaviours observed in such systems , such as information spreading , diffusion , opinion formation and synchronisation , are quite often affected and to some extent determined by the structure of the underlying interaction network  @xcite . \\n however , the recent availability of massive data sets of social , technological and biological systems has suggested that the classical complex network approach might fall somehow short in modelling systems whose elementary units can interact through more than one type of connections . \\n this is typical of many real - world systems , such as social networks , where people are connected through a variety of social relationships , e.g. kinship , friendship , collaboration , competition , or transportation systems , which often exploit different communication channels  @xcite . \\n such systems can be treated in terms of _ multi - layer _ or _ multiplex networks _  \\n @xcite , where each layer describes a particular type of interaction among the nodes of the graph . \\n some recent works have confirmed that multi - layer networks are characterised by new levels of complexity  @xcite and that the interaction of multiple network layers can produce new interesting dynamical behaviours  @xcite .    in the realm of dynamical processes on networks  @xcite the simplicity and -still- \\n the richness of random walks has attracted much attention in recent years  @xcite . \\n random walks are the most simple way to explore a network using only local information , and the steady - state properties of a walk , including characteristic times , limiting occupation probability , and coverage , have tight relationships with the structure of the graph upon which the walk takes place  @xcite . \\n for this reason , random walks have also been successfully used as probes of network properties , with applications ranging from community detection  @xcite to taxonomy of real - world networks  @xcite . moreover , specific flavours of random walks are widely used for the exploration of online social networks , information networks and the like .    \\n a class of random walkers of particular interest is that of walkers whose motion is biased on the structural properties of the network  @xcite . in its simplest possible version , \\n the considered _ biased random walks _ are markov processes whose transition probability is a parametric function of the topological properties of the destination node . in this way , by tuning the parameters of the biasing function one can force the walk to preferentially visit , or avoid , nodes exhibiting high or low values of given topological descriptors , such as the degree , clustering or betweenness . \\n in particular , degree - biased random walks have been used to define new centrality measures  @xcite , identify communities @xcite , and provide optimal exploration of a network using only local information  @xcite . \\n it has also been found that the dynamics of degree - biased random walks is strongly affected by the presence of degree - degree correlations in the structure of the network  @xcite , so that an appropriate choice of the structural bias can be used to perform efficient sampling of unknown networks .    in this article \\n we study several ways in which random walks can be extended to multi - layer networks , and we show how to devise appropriate ways to bias the walkers on the topological properties of the nodes at each layer in order to perform an efficient exploration of such systems . \\n we notice that random walks have already been applied to multi - layer networks , e.g. to quantify the impact of failures in interconnected systems  @xcite . \\n however , we will focus here on biased random walks and will investigate how the biasing function affects the dispersiveness of the walk and the steady - state occupation probability distribution . \\n the aim is to find walks which visit far away regions of a multiplex network within a relatively small number of steps , a property related to the dispersiveness of the walk , and , at the same time , guarantee that the probability for a walker to visit any node in the system is as close as possible to uniform , thus allowing to sample unknown graphs in an efficient way .    \\n the presence of many interdependent layers allows to construct several classes of biased random walks , and in particular what we call _ extensive walks _ and _ intensive walks _ , where the difference between the two classes is in the dependence of the parameters of the biasing function on the number of layers of the system . in the former case , the biasing function depends on the structural properties of the destination node at all the layers of the system ( thus , the number of parameters is extensive in the number of layers ) , while in the latter case the bias depends on intrinsically multiplex properties of the destination node , which do not depend explicitly on the number of layers of the network .    for both classes of biasing functions , we provide analytical closed forms for the long - time properties of \\n the random walks , in terms of stationary probability distribution and entropy rate  @xcite , and we study the effect of different structural properties , including the number of layers , the presence and sign of inter - layer degree correlations , the redundancy of edges across layers , the density of the multiplex and the heterogeneity of the degree distributions , on the steady - state behaviour of these walks . \\n we find that all these properties have a remarkable effect on the maximal dispersiveness and on the steady - state occupation probability of biased random walks \\n .    finally , we study the diffusion properties of several real - world multiplex networks , namely the six continental airline transportation networks , and we show that in those cases the pressure to provide robust route alternatives has somehow hindered the overall diffusion properties of those systems . ', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example in ds:  # example is {'image': tf.Tensor, 'label': tf.Tensor}\n",
    "    abstract = example[\"abstract\"]\n",
    "    article = example[\"article\"]\n",
    "    section_names = example[\"section_names\"]\n",
    "    print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c7739c0-3d4f-4b0e-8443-b6bcea37d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(n:int=100):\n",
    "    val_size = int(n/20)\n",
    "    output_file = 'out_train.csv'\n",
    "    output_file_val = 'out_val.csv'\n",
    "    ds = builder.as_dataset(split=\"train\")\n",
    "    ds = ds.take(n)\n",
    "    print(ds)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"text,summary\\n\")\n",
    "        for example in ds:  # example is {'image': tf.Tensor, 'label': tf.Tensor}\n",
    "            abstract = example[\"abstract\"]\n",
    "            article = example[\"article\"]\n",
    "            f.write('\"'+bytes.decode(article.numpy()).replace('\"', '\\'').replace('\\n', ' ') + '\" ,'+'\"' + bytes.decode(abstract.numpy()).replace('\\n', ' ').replace('\"', '\\'')+'\"'+ \"\\n\")\n",
    "    ds = builder.as_dataset(split=\"validation\")\n",
    "    ds = ds.take(val_size)\n",
    "    with open(output_file_val, 'w') as f:\n",
    "        f.write(\"text,summary\\n\")\n",
    "        for example in ds:  # example is {'image': tf.Tensor, 'label': tf.Tensor}\n",
    "            abstract = example[\"abstract\"]\n",
    "            article = example[\"article\"]\n",
    "            f.write('\"'+bytes.decode(article.numpy()).replace('\"', '\\'').replace('\\n', ' ') + '\" ,'+'\"' + bytes.decode(abstract.numpy()).replace('\\n', ' ').replace('\"', '\\'')+'\"'+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67d31bec-9132-4914-ae5b-77bca0581db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset shapes: {abstract: (), article: (), section_names: ()}, types: {abstract: tf.string, article: tf.string, section_names: tf.string}>\n"
     ]
    }
   ],
   "source": [
    "generate_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63119677-bfc7-4c8a-9770-f10e3202a110",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-08 21:59:46.990483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "12/08/2021 21:59:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "12/08/2021 21:59:48 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/tst-summarization/runs/Dec08_21-59-48_noobzik-B450-AORUS-PRO,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "output_dir=/tmp/tst-summarization,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/tst-summarization,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "12/08/2021 21:59:48 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with `--source_prefix 'summarize: ' `\n",
      "----------------------regukqazerybtfzeiuygftbzeigzeg-----\n",
      "None\n",
      "12/08/2021 21:59:48 - WARNING - datasets.builder - Using custom data configuration default-d457a8cba87d9949\n",
      "12/08/2021 21:59:48 - INFO - datasets.builder - Generating dataset csv (/home/noobzik/.cache/huggingface/datasets/csv/default-d457a8cba87d9949/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
      "Downloading and preparing dataset csv/default to /home/noobzik/.cache/huggingface/datasets/csv/default-d457a8cba87d9949/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 11335.96it/s]\n",
      "12/08/2021 21:59:48 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "12/08/2021 21:59:48 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1591.46it/s]\n",
      "12/08/2021 21:59:48 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "12/08/2021 21:59:48 - INFO - datasets.builder - Generating split train\n",
      "12/08/2021 21:59:48 - INFO - datasets.builder - Generating split validation\n",
      "12/08/2021 21:59:48 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /home/noobzik/.cache/huggingface/datasets/csv/default-d457a8cba87d9949/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1315.45it/s]\n",
      "[INFO|configuration_utils.py:604] 2021-12-08 21:59:49,497 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/noobzik/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:641] 2021-12-08 21:59:49,499 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2021-12-08 21:59:49,880 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:604] 2021-12-08 21:59:50,673 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/noobzik/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:641] 2021-12-08 21:59:50,673 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-12-08 21:59:53,112 >> loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /home/noobzik/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-12-08 21:59:53,113 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /home/noobzik/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-12-08 21:59:53,113 >> loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-12-08 21:59:53,113 >> loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-12-08 21:59:53,113 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:604] 2021-12-08 21:59:53,924 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/noobzik/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "[INFO|configuration_utils.py:641] 2021-12-08 21:59:53,925 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1352] 2021-12-08 21:59:54,415 >> loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /home/noobzik/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "[INFO|modeling_utils.py:1619] 2021-12-08 21:59:55,267 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1627] 2021-12-08 21:59:55,267 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Running tokenizer on train dataset:   0%|                 | 0/1 [00:00<?, ?ba/s]12/08/2021 21:59:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/noobzik/.cache/huggingface/datasets/csv/default-d457a8cba87d9949/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-e0c831e6d4f0d3b0.arrow\n",
      "Running tokenizer on train dataset: 100%|█████████| 1/1 [00:00<00:00,  1.17ba/s]\n",
      "Running tokenizer on validation dataset:   0%|            | 0/1 [00:00<?, ?ba/s]12/08/2021 21:59:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/noobzik/.cache/huggingface/datasets/csv/default-d457a8cba87d9949/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-41ca94ee08981e2d.arrow\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:00<00:00, 14.66ba/s]\n",
      "[INFO|trainer.py:1202] 2021-12-08 22:00:02,114 >> ***** Running training *****\n",
      "[INFO|trainer.py:1203] 2021-12-08 22:00:02,114 >>   Num examples = 100\n",
      "[INFO|trainer.py:1204] 2021-12-08 22:00:02,114 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1205] 2021-12-08 22:00:02,114 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1206] 2021-12-08 22:00:02,114 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1207] 2021-12-08 22:00:02,114 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1208] 2021-12-08 22:00:02,114 >>   Total optimization steps = 75\n",
      "100%|███████████████████████████████████████████| 75/75 [00:10<00:00,  6.77it/s][INFO|trainer.py:1423] 2021-12-08 22:00:13,082 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10.968, 'train_samples_per_second': 27.352, 'train_steps_per_second': 6.838, 'train_loss': 3.293321329752604, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████████| 75/75 [00:10<00:00,  6.85it/s]\n",
      "[INFO|trainer.py:2013] 2021-12-08 22:00:13,085 >> Saving model checkpoint to /tmp/tst-summarization\n",
      "[INFO|configuration_utils.py:425] 2021-12-08 22:00:13,086 >> Configuration saved in /tmp/tst-summarization/config.json\n",
      "[INFO|modeling_utils.py:1070] 2021-12-08 22:00:14,096 >> Model weights saved in /tmp/tst-summarization/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2043] 2021-12-08 22:00:14,098 >> tokenizer config file saved in /tmp/tst-summarization/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2049] 2021-12-08 22:00:14,098 >> Special tokens file saved in /tmp/tst-summarization/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:159] 2021-12-08 22:00:14,144 >> Copy vocab file to /tmp/tst-summarization/spiece.model\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =     3.2933\n",
      "  train_runtime            = 0:00:10.96\n",
      "  train_samples            =        100\n",
      "  train_samples_per_second =     27.352\n",
      "  train_steps_per_second   =      6.838\n",
      "12/08/2021 22:00:14 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:2261] 2021-12-08 22:00:14,154 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2263] 2021-12-08 22:00:14,154 >>   Num examples = 5\n",
      "[INFO|trainer.py:2266] 2021-12-08 22:00:14,154 >>   Batch size = 4\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.64it/s]12/08/2021 22:00:16 - INFO - datasets.metric - Removing /home/noobzik/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.41it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_gen_len            =      119.2\n",
      "  eval_loss               =     3.0311\n",
      "  eval_rouge1             =    32.5507\n",
      "  eval_rouge2             =     8.0205\n",
      "  eval_rougeL             =    23.1237\n",
      "  eval_rougeLsum          =    27.9734\n",
      "  eval_runtime            = 0:00:02.78\n",
      "  eval_samples            =          5\n",
      "  eval_samples_per_second =      1.798\n",
      "  eval_steps_per_second   =      0.719\n",
      "[INFO|modelcard.py:449] 2021-12-08 22:00:17,359 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 32.5507}]}\n"
     ]
    }
   ],
   "source": [
    "!python3 run_summarisation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file out_train.csv \\\n",
    "    --validation_file out_val.csv \\\n",
    "    --output_dir /tmp/tst-summarization \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01e225-ff43-46bd-aa1e-910cc19015f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
